# Hospital-ICU-Mortality-Prediction

## Introduction
In a clinical care setting, the Intensive Care Unit (ICU) is known for having one of the highest in-hospital mortality rates due to the critical nature of its patients. Monitoring the health status of patients in the ICU is crucial for their survival, and accurately assessing their level of health can significantly impact the level of care they receive.
Our final project aims to predict the mortality of patients admitted to the ICU using data from the first 24 hours after admission. The concept of predicting patient severity in the ICU is not new. The Acute Physiology and Chronic Health Evaluation (APACHE) scoring system was first introduced in 1981 by George Washington University and has since become a widely used scale for assessing acute illness. Over the years, several versions of APACHE have been developed, with the latest being APACHE IV, released in 2006. APACHE utilizes physiologic measurements, demographics, and previous health conditions from the first 24 hours to assess the severity of acute illness. While APACHE is an invaluable tool for quick severity assessment using simple body metrics, it does not provide predictions on patient mortality; instead, it aids clinical practitioners in assessing patients.
As data analytics students, our challenge is to predict patient mortality based on 24-hour vital metrics, similar to APACHE, in an attempt to explore the predictive potential of the data. By doing so, we hope to provide a broader perspective for assessing patients in clinical settings and discover the possibilities of leveraging data for mortality prediction.
The dataset encompasses over 130,000 ICU visits worldwide, offering valuable insights such as patient demographics, comorbidity status, laboratory results, and medical measurements taken within the first 24 hours of ICU admission. With a total of 186 columns, it comprises 16 binary, 12 character, and 158 numeric columns. In terms of rows, it contains 91,713 records, and the prediction target variable is the binary outcome of hospital death. The variables included some patient demographic information such as patient id, race, gender, hospital admission types, Body mass index, pre-clinical histories, etc. Vital variables such as diastolic and systolic blood pressure, body temperature, hear rate, white blood cell count, respiratory measure, oxygen, and arterial pH levels are calculated. 

## Data Preprocessing
Due to the high dimensionality of the dataset, instead of scrutinizing each individual variable, this project focused on areas that raised concern. EDA (Exploratory Data analysis) revealed that  some variables having as high as 92% null values. To maintain analysis precision and consider imputation costs, variables with more than 15% null values were dropped. Histogram was used to examine the distribution of variables, and it concluded standardizing variable is essential. Outliers which lack of any logical explanations, such as negative age, are also removed but any other outliers are kept. Thus, the binary target variable is highly imbalanced, only 9% of the target has a value of 1. Over and under sampling was used to balance out the binary target variable. 
An observation was made regarding certain variables appearing to be closely related. Correlation heatmap(refer figure 1 in the final report) indicated strong correlations (correlation coefficient greater than +- 0.7) among specific variables. It is expected to observe such high correlations, especially between variables like minimum and maximum heart rate and blood pressure. Although many variables show positive correlations, variables like eye, motion, and verbal scores exhibit a strong negative correlation with the Apache death probability. To avoid target leakage, we removed all death probabilities from the previous Apache model, as they were highly correlated with the target variable. Additionally, irrelevant ID variables, such as patient and hospital IDs, were also excluded. Regarding categorical variables, gender and race did not demonstrate significant relationships with the target variable. However, they were retained in the model to capture any potentially valuable information.
We combined categorical labels which represent less than 5% of data size. Afterwards we performed releveling to make sure the problem base classes were selected
After the initial preprocessing, the resulting dataset was heavily imbalanced with only around 8% of positive (e.g., resulting in death) cases. To address this, both over and under-sampling was performed to balance the dataset. The newly modified dataset was almost perfectly balanced with 26,369 negative and 26,387 positive.
The next bottleneck faced was the relatively high dimensionality of the dataset. Even after initial preprocessing, the medical records still contained 98 features to account for. To accommodate this, Principal Component Analysis (PCA) was performed on the numerical targets. We chose to retain the first seven Principal Components (PC) which explains 50% of cumulative variance

## Modeling
The first attempt was to fit a Generalized Linear Model (GLM) with binomial family and logit link. The data was partitioned with stratification, allotting 75% to the training set and 25% to the test set. Running this model on the optimized dataset achieved a test accuracy of 0.76 at 0.5 cutoff with test AUC of 0.8547. Additionally, the sensitivity of the model was slightly higher than the specificity. The assessment of these results was that the model had achieved nominal improvement at predicting patient death. The GLM model proved to be achieving its intended purpose. Building upon this, and in pursuit of further improvements, backward feature elimination was performed. This task was performed with the intention of reducing the Akaike Information Criterion (AIC), demonstrating a better balance between bias and variance. Despite these efforts, the improvements were infinitesimal—resulting in a new AUC of 0.8548 (an improvement of only 0.0001). 

The second approach was to fit a tree model. The most influential predictor of the tree was “PC2,” [see Appendix] representing the first split in the branching of the tree.  The use of a Decision Tree model was implemented to explore additional avenues of data mining, The initial Decision Tree was built with a max depth of seven and a Complexity Parameter (CP) of 0.0005. This resulted in a tree containing 47 splits and 48 terminals nodes. The resulting test AUC of the initial tree was 0.8139—a relatively good performance. However, the factors of balance and interpretability needed to be taken in consideration. To produce a more balanced model, the decision tree was pruned at the CP value equal to one standard deviation from the cross-validation error. Now, the tree contained only 37 splits and 38 terminal nodes, making it more easily interpretable. Yet, pruning the tree also resulted in a decrease in the test AUC, bringing it down to 0.8068. It was decided that the minimal sacrifice in accuracy was a prudent trade-off for the comparative gains in balance and simplicity. Thus, the pruned tree was ultimately decided to be the final variant of the model. 

Next approach was to use ensemble models. First, a random forest was fit with 50 trees and assigned a max_depth of seven. This yielded a test AUC of 0.9926. Unfortunately, a score this high is a strong indication of overfitting. 	Upon observation of the variable importance plot of this model, the most influential predictor was identified as “PC2.”

Out final and most successful approach was the SVM. In this project, we encountered the challenge of selecting the appropriate kernel for SVM, and we experimented with both linear and RBF (Radial Basis Function) kernels.For the RBF kernel, we determined the optimal gamma value using the formula 3/k, where k represents the number of features in our analysis [13]. This step was crucial to fine-tune the SVM model and achieve improved accuracy. In our experiments, SVM demonstrated impressive performance, achieving an AUC(Area Under Curve) score of 0.95. SVM outperformed other clustering methods, highlighting the its effectiveness of in tackling complex classification tasks.

## Conclusion
In conclusion, our final project focused on predicting the mortality of patients admitted to the Intensive Care Unit (ICU) using data from the first 24 hours after admission. The dataset, sourced from MIT's GOSSIS database, provided valuable insights into patient demographics, comorbidity status, laboratory results, and vital measurements. Our data analytics exploration began with exploratory data analysis (EDA), where we carefully selected relevant variables, handled missing values, and identified correlations among features.For feature selection, we employed both domain knowledge and statistical methods, seeking to retain important predictors while removing noise and irrelevant variables. Our analysis revealed that principal component 2 (PC2) carried the highest predictive power, indicating the significance of vital metrics and blood pressure in determining patient outcomes. Regression analysis using decision trees and XGBoost not only validated the significance of PC2 but also achieved remarkable R-squared scores and MSE values. While an attempt was made to conduct a random forest analysis, its computational complexity presented challenges during the process. Clustering methods, namely K-means and hierarchical clustering, was attempted to reveal underlying groups of data. Both clustering methods proved less optimal due to the dataset's large size and problem context. In our classification analysis, Support Vector Machine (SVM) emerged as the most powerful algorithm, outperforming other methods with an impressive AUC score of 0.95. SVM's ability to handle non-linear classification tasks showcased its efficacy in dealing with complex medical data.
Our findings demonstrate the value of data mining techniques and machine learning in predicting ICU patient mortality. By leveraging data-driven insights, we gain a deeper understanding of patient severity and can offer more targeted and efficient care. 

NOTE: Please refer to the "final report.pdf" for a detailed explanation of the project. 









