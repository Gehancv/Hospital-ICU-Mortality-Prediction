{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DHG6ktLAQcH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data= pd.read_csv(r\"C:\\Users\\saras\\Documents\\School\\STA 6704- Data Mining 2\\widsdatathon2020\\training_v2.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puUMAOteAq_R"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFXSVGgEA2u-"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4mQ_jxjA8Lx"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "data.isnull().sum()\n",
    "#To be noted that the target variable hospital_death do not have any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOmcKes2A_FD"
   },
   "outputs": [],
   "source": [
    "#Dropping columns causing target leakage\n",
    "data_new= data.drop(['apache_4a_hospital_death_prob','apache_4a_icu_death_prob','heart_rate_apache','d1_heartrate_max','d1_heartrate_min','h1_heartrate_max'],axis=1)\n",
    "\n",
    "#Dropping columns with more than 15% missing values\n",
    "data_new=data_new[data_new.columns[data_new.isnull().sum()<(0.15*data_new.shape[0])]]\n",
    "\n",
    "#Dropping unimportant columns by intuition\n",
    "data_new= data_new.drop(['encounter_id','patient_id','hospital_id'],axis=1)\n",
    "\n",
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkySeC0-BJNV"
   },
   "outputs": [],
   "source": [
    "#Dropping the rows with null values and then check the distribution\n",
    "data_new=data_new.dropna(axis=0)\n",
    "print(data_new.shape)\n",
    "data_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-CB0xnyBU7C"
   },
   "outputs": [],
   "source": [
    "#Dropping the columns with only 0 in them\n",
    "data_new=data_new.drop(['gcs_unable_apache','readmission_status'], axis=1)\n",
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zm1oT7EBYy8"
   },
   "outputs": [],
   "source": [
    "#Checking the different levels and its count\n",
    "col=data_new.select_dtypes(include=['object']).columns\n",
    "for col in col:\n",
    "    print(data_new[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l27R-W8nBmMf"
   },
   "outputs": [],
   "source": [
    "#Combining levels of selected columns\n",
    "data_new['icu_admit_source']=data_new['icu_admit_source'].replace({'Other ICU':'other','Other Hospital':'other'})\n",
    "data_new['apache_2_bodysystem']=data_new['apache_2_bodysystem'].replace({'Undefined diagnoses':'Undefined Diagnoses'})\n",
    "data_new['ethnicity']=data_new['ethnicity'].replace({'Asian':'Other/Unknown','Native American':'Other/Unknown','Hispanic':'Other/Unknown'})\n",
    "\n",
    "col=data_new.select_dtypes(include=['object']).columns\n",
    "for col in col:\n",
    "    print(data_new[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5sJjw6bPcbs"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6Va76JcBx-b"
   },
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLsXLULPB0vw"
   },
   "outputs": [],
   "source": [
    "#Oversampling as the data is imbalanced\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "X = data_new.drop(['hospital_death'], axis=1)\n",
    "y = data_new['hospital_death']\n",
    "over_sampler = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "X_resampled, y_resampled = over_sampler.fit_resample(X, y)\n",
    "oversampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "print(y_resampled.value_counts())\n",
    "oversampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWjLc4freHus"
   },
   "outputs": [],
   "source": [
    "#Splitting data\n",
    "X=oversampled_df.drop('h1_heartrate_min',axis=1)\n",
    "y=oversampled_df['h1_heartrate_min']  #Target variable\n",
    "\n",
    "other_columns_train= X.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "type(other_columns_train)\n",
    "#Adding all the binary and categorical variable to go ahead with encoding\n",
    "other_columns_train=other_columns_train.union(['elective_surgery','apache_post_operative','arf_apache','intubated_apache','ventilated_apache','aids','cirrhosis',\n",
    "'diabetes_mellitus','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis'])\n",
    "print(other_columns_train)\n",
    "#Encoding categorical variables\n",
    "X_encoded=pd.get_dummies(X,columns=other_columns_train,drop_first=True)\n",
    "print(X_encoded.head())\n",
    "print(X_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u7MJN_IIaTx"
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train,X_test,y_train,y_test= train_test_split(X_encoded,y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzMWRm7QLUT3"
   },
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpIbEiKnYVIV"
   },
   "outputs": [],
   "source": [
    "#Scaling the data using standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numeric_columns = X_encoded.select_dtypes(exclude=['uint8']).columns\n",
    "categoric_columns=X_encoded.select_dtypes(include=['uint8']).columns\n",
    "\n",
    "scaled_X=scaler.fit_transform(X_encoded[numeric_columns])\n",
    "\n",
    "\n",
    "                                                                                                                                                      \n",
    "##scaled_X_train=pd.DataFrame(scaled_X_train,columns=X_train[numeric_columns_train].columns)\n",
    "##scaled_X_train = scaled_X_train.reset_index(drop=True)\n",
    "##scaled_X_train= pd.concat([scaled_X_train,X_train[other_columns_train]],axis=1)\n",
    "print(scaled_X.shape)\n",
    "scaled_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNvggoJTfzqa"
   },
   "outputs": [],
   "source": [
    "#SApplyinh PCA for feature generation and dimesnionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "pca = PCA()\n",
    "pca.fit(scaled_X)\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print (var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the cumulative variance by PCs\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(var1) + 1), var1, marker='o', linestyle='-', color='b',markersize=3, linewidth=1.5)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('Cumulative Explained Variance by Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_atw19hiygS"
   },
   "outputs": [],
   "source": [
    "#Selecting variables\n",
    "pca = PCA(n_components=7)\n",
    "pca.fit(scaled_X)\n",
    "X_pc=pca.fit_transform(scaled_X)\n",
    "pc_columns = [f'PC{i+1}' for i in range(X_pc.shape[1])]\n",
    "X_pca = pd.DataFrame(data=X_pc, columns=pc_columns)\n",
    "print(X_pca.shape)\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_encoded[categoric_columns].shape)\n",
    "print(X_pca.shape)\n",
    "X_pca= pd.concat([X_pca.reset_index(drop=True),X_encoded[categoric_columns].reset_index(drop=True)],axis=1,join='outer',ignore_index=False)\n",
    "print(X_pca.shape)\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_pca,X_test_pca,y_train,y_test= train_test_split(X_pca,y,test_size=0.33)\n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting decision tree with hyperparameter tuning using GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "best_dt_params = grid_search.best_params_\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "dt_feature_importance = best_dt_model.feature_importances_\n",
    "dt_feature_importance_df = pd.DataFrame({'Feature': X_train_pca.columns, 'Importance': dt_feature_importance})\n",
    "\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_dt_params)\n",
    "print(\"Best Mean Squared Error:\", abs(grid_search.best_score_))\n",
    "print(\"\\nDecisionTreeRegressor Feature Importance:\")\n",
    "print(dt_feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "y_pred = best_dt_model.predict(X_test_pca)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Decision tree MSE:\", mse)\n",
    "r2score= r2_score(y_test, y_pred)\n",
    "print(\"Decision tree R squared:\",r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features':['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_rf.fit(X_train_pca, y_train)\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "rf_feature_importance = best_rf_model.feature_importances_\n",
    "rf_feature_importance_df = pd.DataFrame({'Feature': X_train_pca.columns, 'Importance': rf_feature_importance})\n",
    "\n",
    "\n",
    "\n",
    "y_pred_rf = best_rf_model.predict(X_test_pca)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(\"Random Forest MSE:\", mse_rf)\n",
    "print(\"\\nRandomForestRegressor Feature Importance:\")\n",
    "print(rf_feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensembling using XGBoost regressor\n",
    "from xgboost import XGBRegressor\n",
    "xgb_regressor = XGBRegressor(random_state=42)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid_xgb, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "y_pred_xgb = best_xgb_model.predict(X_tests)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(\"XGBoost MSE:\", mse_xgb)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
